import os
import fitz  # PyMuPDF for PDF processing
import warnings

from dotenv import load_dotenv
load_dotenv()

# Suppress warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# LangChain and HuggingFace imports
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chat_models import init_chat_model
from langchain.schema import SystemMessage, HumanMessage
from langchain.memory import ConversationBufferMemory
import openai

# --- Configuration ---
PDF_PATH = "Complete English All-in-One for ESL Learners Book.pdf"
TEXT_PATH = "esl_book.txt"
FAISS_FOLDER = "esl_book_faiss_index"


def initialize_rag_pipeline():
    print("--- Initializing RAG components ---")

    api_key = os.getenv("KEY")
    if not api_key:
        raise ValueError("OpenAI API key not found. Please create a .env file with KEY=your-openai-api-key.")
    os.environ["OPENAI_API_KEY"] = api_key

    text = ""
    if os.path.exists(TEXT_PATH):
        print("âœ… Loaded text from existing file.")
        with open(TEXT_PATH, "r", encoding="utf-8") as f:
            text = f.read()
    elif os.path.exists(PDF_PATH):
        print("ğŸ“„ Extracting text from PDF...")
        try:
            doc = fitz.open(PDF_PATH)
            text = "".join([page.get_text() for page in doc])
            doc.close()
            with open(TEXT_PATH, "w", encoding="utf-8") as f:
                f.write(text)
            print("âœ… Text extracted and saved.")
        except Exception as e:
            print(f"Error extracting PDF: {e}")
            raise
    else:
        raise FileNotFoundError(f"PDF file '{PDF_PATH}' not found.")

    print("ğŸ”¹ Splitting text into chunks...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_text(text)
    print(f"Generated {len(chunks)} text chunks.")

    print("ğŸ”„ Loading embedding model...")
    embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    print("âœ… Embedding model loaded.")
    index_file = os.path.join(FAISS_FOLDER, "index.faiss")
    pkl_file = os.path.join(FAISS_FOLDER, "index.pkl")

    if os.path.exists(index_file) and os.path.exists(pkl_file):
        print("ğŸ“¦ Loading existing FAISS index...")
        try:
            db = FAISS.load_local(FAISS_FOLDER, embedding_model, allow_dangerous_deserialization=True)
            print(f"âœ… FAISS index loaded from {FAISS_FOLDER}/")
        except Exception as e:
            print(f"Error loading FAISS index, recreating: {e}")
            db = FAISS.from_texts(chunks, embedding_model)
            db.save_local(FAISS_FOLDER)
            print(f"âœ… New FAISS index created and saved to {FAISS_FOLDER}/")
    else:
        print("ğŸ“¦ Creating and saving new FAISS index...")
        db = FAISS.from_texts(chunks, embedding_model)
        db.save_local(FAISS_FOLDER)
        print(f"âœ… FAISS index saved to {FAISS_FOLDER}/")

    print("ğŸ¤– Initializing chat model...")
    llm = init_chat_model("gpt-3.5-turbo-0125", model_provider="openai")
    print("âœ… Chat model initialized.")

    print("--- RAG components initialized successfully ---")
    return db, llm


# --- Main execution block ---
if __name__ == '__main__':
    db, llm = None, None
    try:
        db, llm = initialize_rag_pipeline()
    except Exception as e:
        print(f"Fatal error during initialization: {e}")
        print("Please resolve the issues and restart the script.")
        exit()

    # Initialize conversation memory
    memory = ConversationBufferMemory(return_messages=True)

    print("\n--- ESL Tutor Ready ---")
    print("â“ Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ ØªØ§ÛŒÙ¾ Ú©Ù†ÛŒØ¯. Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ 'exit' ÛŒØ§ 'Ø®Ø±ÙˆØ¬'.")

    while True:
        query = input("\nâ“ Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯: ").strip()

        if query.lower() in ["exit", "quit", "Ø®Ø±ÙˆØ¬"]:
            print("ğŸ‘‹ Ø®Ø¯Ø§Ø­Ø§ÙØ¸!")
            break

        if not query:
            print("âš ï¸ Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ø³ÙˆØ§Ù„ Ù…Ø¹ØªØ¨Ø± ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.")
            continue

        try:
            print("ğŸ” Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·...")
            docs = db.similarity_search(query, k=4)
            retrieved_content = "\n\n".join(doc.page_content for doc in docs)

            prompt = f"""You are a Persian tutor whose students are Persian.
Use the following excerpts from an ESL textbook to answer the question.
Keep a normal conversation until someone asks you for an English tutorial. Provide a detailed explanation.
Don't explain anything unless they want you to. Answer the question in Persian.
Don't say anything in English unless itâ€™s about teaching something.

Context:
{retrieved_content}

Question:
{query}
"""

            # Load conversation history from memory
            history = memory.load_memory_variables({}).get("history", [])

            # Combine past messages + new human prompt
            messages = history + [HumanMessage(content=prompt)]

            print("ğŸ§  Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ø§Ø² Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ...")
            response = llm(messages)

            # Save new interaction to memory
            memory.save_context(
                {"input": prompt},
                {"output": response.content}
            )

            print("\nâœ… Ù¾Ø§Ø³Ø®:\n", response.content)

        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯: {e}")
            print("Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ø³ÙˆØ§Ù„ Ø¯ÛŒÚ¯Ø±ÛŒ Ø¨Ù¾Ø±Ø³ÛŒØ¯.")
